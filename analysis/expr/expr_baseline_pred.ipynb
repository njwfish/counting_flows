{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f037e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8392daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb2a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "# setup hydra config global for loading this notebook\n",
    "hydra.initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"expr_baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bf3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:30:07 - INFO - Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:30:09 - INFO - Found checkpoint: /orcd/data/omarabu/001/njwfish/counting_flows/outputs/61be507a6200/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 931, Test split: 49\n",
      "Train cells: 1202750, Test cells: 61517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njwfish/miniconda3/envs/seq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import setup_environment, get_checkpoint_info, is_training_complete\n",
    "\n",
    "# Setup environment\n",
    "device = setup_environment(cfg)\n",
    "\n",
    "# Get checkpoint info\n",
    "output_dir, checkpoint = get_checkpoint_info(cfg, num_epochs=cfg.training.num_epochs)\n",
    "\n",
    "# Instantiate everything\n",
    "bridge = hydra.utils.instantiate(cfg.bridge)\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "avg_model = hydra.utils.instantiate(cfg.averaging, model=model)\n",
    "avg_model.load_state_dict(checkpoint['avg_model_state_dict'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091ff9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "avg_model = hydra.utils.instantiate(cfg.averaging, model=model)\n",
    "avg_model.load_state_dict(checkpoint['avg_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7156877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ctx_window_size    = 196_608\n",
    "target_window_size = 896\n",
    "stride             = 896   # tiles exactly; use <= 896 to allow overlap\n",
    "\n",
    "whalf = ctx_window_size // 2\n",
    "thalf = target_window_size // 2\n",
    "\n",
    "base_idx = dataset.base_individual_idx  # (n_cells,)\n",
    "device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "gene_counts = {}\n",
    "for ci, gs, ge, gn in zip(dataset._eligible_cidx, dataset._eligible_start, dataset._eligible_end, dataset._eligible_gene_name):\n",
    "\n",
    "    chrom_beg  = dataset.chrom_starts[ci]\n",
    "    gene_start = int(chrom_beg + int(gs))\n",
    "    gene_end   = int(chrom_beg + int(ge))  # end-exclusive\n",
    "    gene_len   = int(gene_end - gene_start)\n",
    "\n",
    "    print(gene_len)\n",
    "\n",
    "    big_seq = dataset.get_seq(gene_start - whalf, gene_end + whalf)\n",
    "    assert len(big_seq) == gene_len + 2*whalf\n",
    "\n",
    "    # Centers whose 896bp target is fully inside the gene\n",
    "    first_center = thalf\n",
    "    last_center  = gene_len - thalf\n",
    "    # if last_center < first_center:\n",
    "    #     continue  # gene shorter than target; skip or handle separately\n",
    "\n",
    "    centers_local = np.arange(first_center, last_center + 1, stride, dtype=np.int64)\n",
    "    if len(centers_local) == 0:\n",
    "        centers_local = np.array([first_center])\n",
    "    # ensure we always include a tail center that lands exactly on the end\n",
    "    if centers_local[-1] != last_center and centers_local[-1] != first_center:\n",
    "        centers_local = np.append(centers_local, last_center)\n",
    "\n",
    "    # Precompute per-cell coverage over the whole gene once; weâ€™ll slice per window\n",
    "    # (shape: [n_cells, gene_len])\n",
    "    # gene_end_extended shuold be a multiple of 896\n",
    "    gene_end_extended = gene_start + ((gene_len - 1) // 896 + 1) * 896\n",
    "\n",
    "    # Output buffer: counts per cell along the gene\n",
    "    count_arr = np.zeros((1, gene_len), dtype=np.int32)\n",
    "\n",
    "    for c in centers_local:\n",
    "        # Context sequence for this center\n",
    "        seq = big_seq[c : c + ctx_window_size]\n",
    "        # Target genomic slice indices in gene-local coords\n",
    "        g0 = int(c - thalf)\n",
    "        g1 = g0 + target_window_size\n",
    "\n",
    "\n",
    "        # Build context dict for your model\n",
    "        context = {\n",
    "            'seq': torch.from_numpy(seq).to(device),\n",
    "            'class_emb': torch.zeros(\n",
    "                1, dataset.target_cond.shape[1], device=device\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # ---- your model call (unchanged) ----\n",
    "        with torch.no_grad():\n",
    "            target_counts = bridge.sampler(\n",
    "                torch.zeros(1, 896, device=device),\n",
    "                context,\n",
    "                (avg_model.module if avg_model is not None else model).to(device),\n",
    "                n_steps=3,\n",
    "            )  # expect [n_cells, 896]\n",
    "\n",
    "        # Write into the gene buffer\n",
    "        count_arr[:, g0:g1] = target_counts.to(\"cpu\").numpy()[:, :gene_len]\n",
    "\n",
    "    gene_counts[gn] = count_arr\n",
    "\n",
    "# save gene_counts with pickle\n",
    "import pickle as pkl\n",
    "pkl.dump(gene_counts, open(\"results/expr/gene_counts_baseline.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc16f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n",
      "523\n",
      "7098\n",
      "26277\n",
      "5077\n",
      "7105\n",
      "36280\n",
      "37494\n",
      "2396\n",
      "21619\n",
      "4522\n",
      "7814\n",
      "28181\n",
      "4671\n",
      "6729\n",
      "4514\n",
      "2784\n",
      "9089\n",
      "7508\n",
      "21570\n",
      "5127\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ctx_window_size    = 196_608\n",
    "target_window_size = 896\n",
    "stride             = 896   # tiles exactly; use <= 896 to allow overlap\n",
    "\n",
    "whalf = ctx_window_size // 2\n",
    "thalf = target_window_size // 2\n",
    "\n",
    "base_idx = dataset.base_individual_idx  # (n_cells,)\n",
    "device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "gene_counts = {}\n",
    "for cell_type in range(dataset.target_cond.shape[1]):\n",
    "    gene_counts[cell_type] = {}\n",
    "    for ci, gs, ge, gn in zip(dataset._eligible_cidx, dataset._eligible_start, dataset._eligible_end, dataset._eligible_gene_name):\n",
    "\n",
    "        chrom_beg  = dataset.chrom_starts[ci]\n",
    "        gene_start = int(chrom_beg + int(gs))\n",
    "        gene_end   = int(chrom_beg + int(ge))  # end-exclusive\n",
    "        gene_len   = int(gene_end - gene_start)\n",
    "\n",
    "        print(gene_len)\n",
    "\n",
    "        big_seq = dataset.get_seq(gene_start - whalf, gene_end + whalf)\n",
    "        assert len(big_seq) == gene_len + 2*whalf\n",
    "\n",
    "        # Centers whose 896bp target is fully inside the gene\n",
    "        first_center = thalf\n",
    "        last_center  = gene_len - thalf\n",
    "        # if last_center < first_center:\n",
    "        #     continue  # gene shorter than target; skip or handle separately\n",
    "\n",
    "        centers_local = np.arange(first_center, last_center + 1, stride, dtype=np.int64)\n",
    "        if len(centers_local) == 0:\n",
    "            centers_local = np.array([first_center])\n",
    "        # ensure we always include a tail center that lands exactly on the end\n",
    "        if centers_local[-1] != last_center and centers_local[-1] != first_center:\n",
    "            centers_local = np.append(centers_local, last_center)\n",
    "\n",
    "        # Precompute per-cell coverage over the whole gene once; weâ€™ll slice per window\n",
    "        # (shape: [n_cells, gene_len])\n",
    "        # gene_end_extended shuold be a multiple of 896\n",
    "        gene_end_extended = gene_start + ((gene_len - 1) // 896 + 1) * 896\n",
    "\n",
    "        # Output buffer: counts per cell along the gene\n",
    "        count_arr = np.zeros((base_idx.shape[0], gene_len), dtype=np.int32)\n",
    "\n",
    "        for c in centers_local:\n",
    "            # Context sequence for this center\n",
    "            seq = big_seq[c : c + ctx_window_size]\n",
    "            # Target genomic slice indices in gene-local coords\n",
    "            g0 = int(c - thalf)\n",
    "            g1 = g0 + target_window_size\n",
    "\n",
    "\n",
    "            context = {\n",
    "                'seq': torch.from_numpy(seq).to(device),\n",
    "                'class_emb': torch.zeros(\n",
    "                    1, dataset.target_cond.shape[1], device=device\n",
    "                )\n",
    "            }\n",
    "\n",
    "            context['class_emb'][:, cell_type] = 1\n",
    "\n",
    "            # ---- your model call (unchanged) ----\n",
    "            with torch.no_grad():\n",
    "                target_counts = bridge.sampler(\n",
    "                    torch.zeros(1, 896, device=device),\n",
    "                    context,\n",
    "                    (avg_model.module if avg_model is not None else model).to(device),\n",
    "                    n_steps=3,\n",
    "                )  # expect [n_cells, 896]\n",
    "\n",
    "            # Write into the gene buffer\n",
    "            count_arr[:, g0:g1] = target_counts.to(\"cpu\").numpy()[:, :gene_len]\n",
    "\n",
    "        gene_counts[cell_type][gn] = count_arr\n",
    "\n",
    "# save gene_counts with pickle\n",
    "import pickle as pkl\n",
    "pkl.dump(gene_counts, open(\"results/expr/cell_type_gene_counts_baseline.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
