{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f037e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8392daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb2a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "# setup hydra config global for loading this notebook\n",
    "hydra.initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = hydra.compose(config_name=\"expr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:36:40 - INFO - Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:36:42 - INFO - Found checkpoint: /orcd/data/omarabu/001/njwfish/counting_flows/outputs/84cddc73ed43/model.pt\n",
      "/home/njwfish/miniconda3/envs/seq/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 931, Test split: 49\n",
      "Train cells: 1202750, Test cells: 61517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from main import setup_environment, get_checkpoint_info, is_training_complete\n",
    "\n",
    "# Setup environment\n",
    "device = setup_environment(cfg)\n",
    "\n",
    "# Get checkpoint info\n",
    "output_dir, checkpoint = get_checkpoint_info(cfg, num_epochs=cfg.training.num_epochs)\n",
    "\n",
    "# Instantiate everything\n",
    "bridge = hydra.utils.instantiate(cfg.bridge)\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "avg_model = hydra.utils.instantiate(cfg.averaging, model=model)\n",
    "avg_model.load_state_dict(checkpoint['avg_model_state_dict'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ff9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "avg_model = hydra.utils.instantiate(cfg.averaging, model=model)\n",
    "avg_model.load_state_dict(checkpoint['avg_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58418809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnergyScoreLoss(\n",
       "  (architecture): SCFormer(\n",
       "    (enformer): Enformer(\n",
       "      (stem): Sequential(\n",
       "        (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv_tower): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transformer): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (crop_final): TargetLengthCrop()\n",
       "      (final_pointwise): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Rearrange('b d n -> b n d')\n",
       "        (3): Dropout(p=0.05, inplace=False)\n",
       "        (4): GELU()\n",
       "      )\n",
       "      (_trunk): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (1): GELU()\n",
       "                (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): AttentionPool(\n",
       "              (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "              (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Rearrange('b d n -> b n d')\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): Sequential(\n",
       "            (0): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Attention(\n",
       "                  (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                  (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                  (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                  (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                  (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Residual(\n",
       "              (fn): Sequential(\n",
       "                (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "                (2): Dropout(p=0.4, inplace=False)\n",
       "                (3): ReLU()\n",
       "                (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "                (5): Dropout(p=0.4, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): TargetLengthCrop()\n",
       "        (6): Sequential(\n",
       "          (0): Rearrange('b n d -> b d n')\n",
       "          (1): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (2): Rearrange('b d n -> b n d')\n",
       "          (3): Dropout(p=0.05, inplace=False)\n",
       "          (4): GELU()\n",
       "        )\n",
       "      )\n",
       "      (_heads): ModuleDict(\n",
       "        (human): Sequential(\n",
       "          (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "          (1): Softplus(beta=1.0, threshold=20.0)\n",
       "        )\n",
       "        (mouse): Sequential(\n",
       "          (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "          (1): Softplus(beta=1.0, threshold=20.0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (in_proj): Sequential(\n",
       "      (0): Linear(in_features=3188, out_features=256, bias=True)\n",
       "      (1): SELU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (norm_layers): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_proj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (in_attn_proj_proj): Sequential(\n",
       "      (0): Linear(in_features=259, out_features=256, bias=True)\n",
       "      (1): SELU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (attn_proj_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (norm_proj_layers): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_proj_proj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc16f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "794_795 523 896 True\n",
      "counts match True\n",
      "7098\n",
      "794_795 7098 7168 True\n",
      "counts match True\n",
      "26277\n",
      "794_795 26277 26880 True\n",
      "counts match True\n",
      "5077\n",
      "794_795 5077 5376 True\n",
      "counts match True\n",
      "7105\n",
      "794_795 7105 7168 True\n",
      "counts match True\n",
      "36280\n",
      "794_795 36280 36736 True\n",
      "counts match True\n",
      "37494\n",
      "794_795 37494 37632 True\n",
      "counts match True\n",
      "2396\n",
      "794_795 2396 2688 True\n",
      "counts match True\n",
      "21619\n",
      "794_795 21619 22400 True\n",
      "counts match True\n",
      "4522\n",
      "794_795 4522 5376 True\n",
      "counts match True\n",
      "7814\n",
      "794_795 7814 8064 True\n",
      "counts match True\n",
      "28181\n",
      "794_795 28181 28672 True\n",
      "counts match True\n",
      "4671\n",
      "794_795 4671 5376 True\n",
      "counts match True\n",
      "6729\n",
      "794_795 6729 7168 True\n",
      "counts match True\n",
      "4514\n",
      "794_795 4514 5376 True\n",
      "counts match True\n",
      "2784\n",
      "794_795 2784 3584 True\n",
      "counts match True\n",
      "9089\n",
      "794_795 9089 9856 True\n",
      "counts match True\n",
      "7508\n",
      "794_795 7508 8064 True\n",
      "counts match True\n",
      "21570\n",
      "794_795 21570 22400 True\n",
      "counts match True\n",
      "5127\n",
      "794_795 5127 5376 True\n",
      "counts match True\n",
      "523\n",
      "93_93 523 896 True\n",
      "counts match True\n",
      "7098\n",
      "93_93 7098 7168 True\n",
      "counts match True\n",
      "26277\n",
      "93_93 26277 26880 True\n",
      "counts match True\n",
      "5077\n",
      "93_93 5077 5376 True\n",
      "counts match True\n",
      "7105\n",
      "93_93 7105 7168 True\n",
      "counts match True\n",
      "36280\n",
      "93_93 36280 36736 True\n",
      "counts match True\n",
      "37494\n",
      "93_93 37494 37632 True\n",
      "counts match True\n",
      "2396\n",
      "93_93 2396 2688 True\n",
      "counts match True\n",
      "21619\n",
      "93_93 21619 22400 True\n",
      "counts match True\n",
      "4522\n",
      "93_93 4522 5376 True\n",
      "counts match True\n",
      "7814\n",
      "93_93 7814 8064 True\n",
      "counts match True\n",
      "28181\n",
      "93_93 28181 28672 True\n",
      "counts match True\n",
      "4671\n",
      "93_93 4671 5376 True\n",
      "counts match True\n",
      "6729\n",
      "93_93 6729 7168 True\n",
      "counts match True\n",
      "4514\n",
      "93_93 4514 5376 True\n",
      "counts match True\n",
      "2784\n",
      "93_93 2784 3584 True\n",
      "counts match True\n",
      "9089\n",
      "93_93 9089 9856 True\n",
      "counts match True\n",
      "7508\n",
      "93_93 7508 8064 True\n",
      "counts match True\n",
      "21570\n",
      "93_93 21570 22400 True\n",
      "counts match True\n",
      "5127\n",
      "93_93 5127 5376 True\n",
      "counts match True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ctx_window_size    = 196_608\n",
    "target_window_size = 896\n",
    "stride             = 896   # tiles exactly; use <= 896 to allow overlap\n",
    "\n",
    "whalf = ctx_window_size // 2\n",
    "thalf = target_window_size // 2\n",
    "\n",
    "base_idx = dataset.base_individual_idx  # (n_cells,)\n",
    "target_idxs = dataset.test_individual_idxs\n",
    "\n",
    "device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "\n",
    "# gene_counts = {}\n",
    "# gene_counts_true = {}\n",
    "for target_key in target_idxs:\n",
    "    if target_key in gene_counts:\n",
    "        continue\n",
    "    gene_counts[target_key] = {}\n",
    "    gene_counts_true[target_key] = {}\n",
    "    target_idx = target_idxs[target_key]\n",
    "    try:    \n",
    "        for ci, gs, ge, gn in zip(dataset._eligible_cidx, dataset._eligible_start, dataset._eligible_end, dataset._eligible_gene_name):\n",
    "\n",
    "            chrom_beg  = dataset.chrom_starts[ci]\n",
    "            gene_start = int(chrom_beg + int(gs))\n",
    "            gene_end   = int(chrom_beg + int(ge))  # end-exclusive\n",
    "            gene_len   = int(gene_end - gene_start)\n",
    "\n",
    "            print(gene_len)\n",
    "\n",
    "            big_seq = dataset.get_seq(gene_start - whalf, gene_end + whalf)\n",
    "            assert len(big_seq) == gene_len + 2*whalf\n",
    "\n",
    "            # Centers whose 896bp target is fully inside the gene\n",
    "            first_center = thalf\n",
    "            last_center  = gene_len - thalf\n",
    "            # if last_center < first_center:\n",
    "            #     continue  # gene shorter than target; skip or handle separately\n",
    "\n",
    "            centers_local = np.arange(first_center, last_center + 1, stride, dtype=np.int64)\n",
    "            if len(centers_local) == 0:\n",
    "                centers_local = np.array([first_center])\n",
    "            # ensure we always include a tail center that lands exactly on the end\n",
    "            if centers_local[-1] != last_center and centers_local[-1] != first_center:\n",
    "                centers_local = np.append(centers_local, last_center)\n",
    "\n",
    "            # Precompute per-cell coverage over the whole gene once; we’ll slice per window\n",
    "            # (shape: [n_cells, gene_len])\n",
    "            # gene_end_extended shuold be a multiple of 896\n",
    "            gene_end_extended = gene_start + ((gene_len - 1) // 896 + 1) * 896\n",
    "            print(target_key, gene_end - gene_start, gene_end_extended - gene_start, gene_end < gene_end_extended)\n",
    "            x_1_global = torch.from_numpy(\n",
    "                dataset.fast_get_overlap_raw(base_idx[:target_idx.shape[0]], (gene_start, gene_end_extended))\n",
    "            )# .to(device)\n",
    "\n",
    "            gene_counts_true[target_key][gn] = dataset.fast_get_overlap_raw(target_idx, (gene_start, gene_end_extended))\n",
    "            target_sum_global = torch.from_numpy(gene_counts_true[target_key][gn]).sum(dim=0)\n",
    "            gene_counts_true[target_key][gn] = gene_counts_true[target_key][gn][:,:gene_len]\n",
    "\n",
    "            # Output buffer: counts per cell along the gene\n",
    "            count_arr = np.zeros((target_idx.shape[0], gene_len), dtype=np.int32)\n",
    "\n",
    "            for c in centers_local:\n",
    "                # Context sequence for this center\n",
    "                seq = big_seq[c : c + ctx_window_size]\n",
    "                # Target genomic slice indices in gene-local coords\n",
    "                g0 = int(c - thalf)\n",
    "                g1 = g0 + target_window_size\n",
    "\n",
    "\n",
    "                # Slice the “conditioning” counts for these positions\n",
    "                x_1 = x_1_global[:, g0:g1].to(device)  # [n_cells, 896]\n",
    "                # Build context dict for your model\n",
    "                context = {\n",
    "                    'seq': torch.from_numpy(seq).to(device),\n",
    "                    'class_emb': torch.zeros(\n",
    "                        target_idx.shape[0], dataset.target_cond.shape[1], device=device\n",
    "                    ),\n",
    "                    'target_sum': target_sum_global[g0:g1].unsqueeze(0).to(device),\n",
    "                    'A': torch.ones(1, target_idx.shape[0], device=device).to(device)\n",
    "                }\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_counts = bridge.sampler(\n",
    "                        x_1,\n",
    "                        context,\n",
    "                        (avg_model.module if avg_model is not None else model).to(device),\n",
    "                        n_steps=3,\n",
    "                    )  # expect [n_cells, 896]\n",
    "\n",
    "                # Write into the gene buffer\n",
    "                count_arr[:, g0:g1] = target_counts.to(\"cpu\").numpy()[:, :gene_len]\n",
    "\n",
    "            gene_counts[target_key][gn] = count_arr\n",
    "            print(\"counts match\",np.all(count_arr.sum(axis=0) == gene_counts_true[target_key][gn].sum(axis=0)))\n",
    "    except Exception as e:\n",
    "        import gc\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        continue\n",
    "    # save gene_counts with pickle\n",
    "    import pickle as pkl\n",
    "    pkl.dump(gene_counts, open(\"results/expr/deconv_gene_counts_proj.pkl\", \"wb\"))\n",
    "    # pkl.dump(gene_counts_true, open(\"results/expr/deconv_gene_counts_true.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4075a7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2674"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ddb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
